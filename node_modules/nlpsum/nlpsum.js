var natural = require('natural');
var _ 		= require('underscore');
var sentenceParser = require('./nlp-node-master/sentence_parser/sentence');


function nlpsum() {
	this.nounInflector = new natural.NounInflector();
}

nlpsum.prototype.fractal = function(text, quota) {
	var scope = this;
	
	var tokenizer 	= new natural.WordTokenizer();
	
	var threshold	= 0.18;
	
	var summary 	= [];
	
	var fractal = {
		paragraphs:	[],
		weights:	scope.tf(text).frequency,
	};
	// Split in paragraphs
	var paragraphs = text.split('\r\n\r\n');
	
	var s_weight = 0;	// Total weights of all the sentences in reference to the document. Will be > 1. Used to normalize to the range ]0;1].
	var p_weight = 0;	// Total weights of all the paragraphs in reference to the document. Will be > 1. Used to normalize to the range ]0;1].
	
	_.each(paragraphs, function(paragraph) {
		// Temp object we'll push
		var buffer_p = {
			paragraph:	paragraph,
			tokens:		scope.tokenize(paragraph),
			weights:	{
				words:	scope.tf(paragraph).frequency,
				total:	{
					words:		0,
					sentences:	0
				}
			},
			sentences:	[]
		};
		
		// Split in sentences
		var sentences = sentenceParser(paragraph);
		
		_.each(sentences, function(sentence) {
			var buffer_s = {
				sentence: 	sentence,
				tokens:		scope.tokenize(sentence)
			};
			
			if (buffer_s.tokens.length > 0) {
				buffer_s.weights = {
					sentence:	scope.tf(sentence).frequency,
					paragraph:	scope.reltf(buffer_s.tokens, buffer_p.weights),
					document:	scope.reltf(buffer_s.tokens, fractal.weights)
				};
				buffer_s.weights.total = {
					sentence:	1,	// always, logical
					paragraph:	_.reduce(_.values(buffer_s.weights.paragraph), function(total, n) {return total+n;}, 0),
					document:	_.reduce(_.values(buffer_s.weights.document), function(total, n) {return total+n;}, 0)
				}
				
				s_weight += buffer_s.weights.total.document;
				buffer_p.weights.total.sentences += buffer_s.weights.total.document;
				
				buffer_p.sentences.push(buffer_s);
				
				/*
				// Old way, using manually set threshold
				if (buffer_s.weights.total.document > threshold) {
					summary.push(sentence.trim());
				}
				*/
			}
		});
		// Calculate tht total weights of the words in that paragraph, relative to the document
		buffer_p.weights.total.words = _.reduce(_.values(buffer_p.weights.words), function(total, n) {return total+n;}, 0);
		
		
		fractal.paragraphs.push(buffer_p)
	});
	
	// Now we normalize the sentence weights, using the sum 's_weight'
	var ns_weight = 0;
	var np_weight = 0;
	var quota_sum = 0;
	this.each(fractal.paragraphs, function(paragraph) {
		// Normalize the paragraph weight relative to the document ]0;1]
		paragraph.weights.total.normalized = paragraph.weights.total.sentences/s_weight;
		np_weight += paragraph.weights.total.normalized;
		
		// Calculate the quota for that paragraph
		paragraph.quota = Math.round(quota*paragraph.weights.total.normalized);
		quota_sum += paragraph.quota;
		
		// Now that we have the quotaz per paragraph and the normalized weights (actually, next step),
		// We define the thereshold per paragraph
		// To do that, we sort the sentence by weight
		var sentences_sorted = [];
		
		var index = 0;
		scope.each(paragraph.sentences, function(sentence) {
			// Normalize the sentence weight relative to the document ]0;1]
			sentence.weights.total.sentence_normalized = sentence.weights.total.document/s_weight;
			ns_weight += sentence.weights.total.sentence_normalized;
			
			sentences_sorted.push({
				weight:	sentence.weights.total.sentence_normalized,
				text:	sentence.sentence,
				index:	index
			});
			index++;
		});
		
		// Sort the sentences by weight
		paragraph.sentences_sorted 	= sentences_sorted.sort(function(a,b) {return b.weight-a.weight;});
		
		// Keep only our quotas of sentences, sorted by index asc (to be in the right order)
		paragraph.sentences_keep 	= _.map(paragraph.sentences_sorted.slice(0, paragraph.quota).sort(function(a,b) {return a.index-b.index;}), function(item) {
			return item.text;
		});
		
		if (paragraph.sentences_keep.length > 0) {
			_.each(paragraph.sentences_keep, function(sentence) {
				summary.push(sentence.trim());
			});
		}
		
		
		
	});
	
	return {
		data:		fractal,
		text:		summary.join('.\r\n'),
		s_weight:	s_weight,
		ns_weight:	ns_weight,
		np_weight:	np_weight,
		quota_sum:	quota_sum
	};
}
nlpsum.prototype.each = function(array, fn) {
	var i;
	var l = array.length;
	for (i=0;i<l;i++) {
		fn(array[i]);
	}
}

// Relative text frequency
nlpsum.prototype.reltf = function(sentenceTokens, textWeights) {
	var scope = this;
	
	var frequency = {};
	
	_.each(textWeights, function(weight, token) {
		if (_.contains(sentenceTokens, token)) {
			frequency[token] = weight;
		}
	});
	
	return frequency;
}


nlpsum.prototype.split = function(text) {
	var scope = this;
	
	var tokenizer 	= new natural.WordTokenizer();
	
	var fractal = {
		paragraphs:	[]
	};
	// Split in paragraphs
	var paragraphs = text.split('\r\n\r\n');
	
	_.each(paragraphs, function(paragraph) {
		// Temp object we'll push
		var buffer_p = {
			paragraph:	paragraph,
			tokens:		scope.tokenize(paragraph),
			sentences:	[]
		};
		
		// Split in sentences
		var sentences = paragraph.split('.');
		
		_.each(sentences, function(sentence) {
			var buffer_s = {
				sentence: 	sentence,
				tokens:		scope.tokenize(sentence)
			};
			if (buffer_s.tokens.length > 0) {
				buffer_p.sentences.push(buffer_s);
			}
		});
		
		fractal.paragraphs.push(buffer_p)
	});
	return fractal;
}

// Tokenize a text, removing the plurals and removing the useless words (a, the, ...)
nlpsum.prototype.tokenize = function(text) {
	var scope = this;
	var tokenizer 	= new natural.WordTokenizer();
	var tokens 		= tokenizer.tokenize(text);
	
	// Lowercase
	tokens = _.map(tokens, function(token) {
		return token.toString().toLowerCase();
	});
	
	// Remove the useless words
	var removeList 	= ["a","the","of","it","he","she","we","our","they","from","to","that","this","is","in","these","be","at","s","re","and","or","with","which","what","was"];
	tokens = _.filter(tokens, function(token) {
		return !_.contains(removeList, token);
	});
	/*
	tokens = _.map(tokens, function(token) {
		return scope.nounInflector.singularize(token);
	});*/
	return tokens;
}
nlpsum.prototype.tf = function(text) {
	var scope = this;
	
	// Tokenize the text
	var tokens			= this.tokenize(text);
	
	// Calculate the frequency
	var count 		= {};
	var sum			= 0;
	var frequency 	= {};
	_.each(tokens, function(token) {
		if (!_.has(count, token)) {
			count[token] = 0;
			sum++;
		}
		count[token]++;
	});
	var token;
	for (token in count) {
		frequency[token] = count[token]/sum;
	}
	
	return {
		tokens:		tokens,
		frequency:	frequency,
		count:		count,
		sum:		sum
	};
}












exports.version		= "1.5.2";
exports.main		= nlpsum;