var natural = require('natural');
var _ 		= require('underscore');


function nlpsum() {
	this.nounInflector = new natural.NounInflector();
}
nlpsum.prototype.fractal = function(text, quota) {
	var scope = this;
	
	var tokenizer 	= new natural.WordTokenizer();
	
	var threshold	= 0.18;
	
	var summary 	= [];
	
	var fractal = {
		paragraphs:	[],
		weights:	scope.tf(text).frequency,
	};
	// Split in paragraphs
	var paragraphs = text.split('\r\n\r\n');
	
	_.each(paragraphs, function(paragraph) {
		// Temp object we'll push
		var buffer_p = {
			paragraph:	paragraph,
			tokens:		scope.tokenize(paragraph),
			weights:	scope.tf(paragraph).frequency,
			sentences:	[]
		};
		
		// Split in sentences
		var sentences = paragraph.split('.');
		
		_.each(sentences, function(sentence) {
			var buffer_s = {
				sentence: 	sentence,
				tokens:		scope.tokenize(sentence)
			};
			
			if (buffer_s.tokens.length > 0) {
				buffer_s.weights = {
					sentence:	scope.tf(sentence).frequency,
					paragraph:	scope.reltf(buffer_s.tokens, buffer_p.weights),
					document:	scope.reltf(buffer_s.tokens, fractal.weights)
				};
				buffer_s.weights.total = {
					sentence:	1,	// always, logical
					paragraph:	_.reduce(_.values(buffer_s.weights.paragraph), function(total, n) {return total+n;}, 0),
					document:	_.reduce(_.values(buffer_s.weights.document), function(total, n) {return total+n;}, 0)
				}
				buffer_p.sentences.push(buffer_s);
				
				if (buffer_s.weights.total.document > threshold) {
					summary.push(sentence.trim());
				}
			}
		});
		
		fractal.paragraphs.push(buffer_p)
	});
	
	delete fractal.weights;
	
	return summary.join('.\r\n');
}

// Relative text frequency
nlpsum.prototype.reltf = function(sentenceTokens, textWeights) {
	var scope = this;
	
	var frequency = {};
	
	_.each(textWeights, function(weight, token) {
		if (_.contains(sentenceTokens, token)) {
			frequency[token] = weight;
		}
	});
	
	return frequency;
}


nlpsum.prototype.split = function(text) {
	var scope = this;
	
	var tokenizer 	= new natural.WordTokenizer();
	
	var fractal = {
		paragraphs:	[]
	};
	// Split in paragraphs
	var paragraphs = text.split('\r\n\r\n');
	
	_.each(paragraphs, function(paragraph) {
		// Temp object we'll push
		var buffer_p = {
			paragraph:	paragraph,
			tokens:		scope.tokenize(paragraph),
			sentences:	[]
		};
		
		// Split in sentences
		var sentences = paragraph.split('.');
		
		_.each(sentences, function(sentence) {
			var buffer_s = {
				sentence: 	sentence,
				tokens:		scope.tokenize(sentence)
			};
			if (buffer_s.tokens.length > 0) {
				buffer_p.sentences.push(buffer_s);
			}
		});
		
		fractal.paragraphs.push(buffer_p)
	});
	return fractal;
}

// Tokenize a text, removing the plurals and removing the useless words (a, the, ...)
nlpsum.prototype.tokenize = function(text) {
	var scope = this;
	var tokenizer 	= new natural.WordTokenizer();
	var tokens 		= tokenizer.tokenize(text);
	
	// Lowercase
	tokens = _.map(tokens, function(token) {
		return token.toString().toLowerCase();
	});
	
	// Remove the useless words
	var removeList 	= ["a","the","of","it","he","she","we","our","they","from","to","that","this","is","in","these","be","at","s","re","and","or","with","which","what","was"];
	tokens = _.filter(tokens, function(token) {
		return !_.contains(removeList, token);
	});
	/*
	tokens = _.map(tokens, function(token) {
		return scope.nounInflector.singularize(token);
	});*/
	return tokens;
}
nlpsum.prototype.tf = function(text) {
	var scope = this;
	
	// Tokenize the text
	var tokens			= this.tokenize(text);
	
	// Calculate the frequency
	var count 		= {};
	var sum			= 0;
	var frequency 	= {};
	_.each(tokens, function(token) {
		if (!_.has(count, token)) {
			count[token] = 0;
			sum++;
		}
		count[token]++;
	});
	var token;
	for (token in count) {
		frequency[token] = count[token]/sum;
	}
	
	return {
		tokens:		tokens,
		frequency:	frequency,
		count:		count,
		sum:		sum
	};
}












exports.version		= "1.5.2";
exports.main		= nlpsum;